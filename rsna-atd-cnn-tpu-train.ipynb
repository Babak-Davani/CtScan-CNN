{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RSNA 2023 Abdominal Trauma Detection](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection)\n",
    "\n",
    "> Detect and classify traumatic abdominal injuries\n",
    "\n",
    "![](https://www.kaggle.com/competitions/52254/images/header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "* In this notebook we will classify `Detect and classify traumatic abdominal injuries` from CT scans.\n",
    "* For this ntoebook, we'll use classic **CNN** model: EfficientNet for our training.\n",
    "* **Wandb** is integrated hence we can use this notebook to track which experiemnt is peforming better.\n",
    "* We'll maximize the `binary_cross_entropy` score using as it similar to `log_loss`.\n",
    "\n",
    "> This notebook uses similar methodology as my previous notebook [RSNA-BCD: EfficientNet](https://www.kaggle.com/code/awsaf49/rsna-bcd-efficientnet-tf-tpu-1vm-train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebooks\n",
    "* CNN:\n",
    "    * Train: [RSNA-ATD: CNN [TPU][Train]](https://www.kaggle.com/awsaf49/rsna-atd-cnn-tpu-train/)\n",
    "    * Infer: [RSNA-ATD: CNN [TPU][Infer]](https://www.kaggle.com/awsaf49/rsna-atd-cnn-tpu-infer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Augmentations:\n",
    "* Random - Horizontal Flip\n",
    "* Random - Brightness, Contrast, Hue, Saturation\n",
    "* Coarse Dropout\n",
    "* MixUp, CutMix\n",
    "\n",
    "## WandB Integration:\n",
    "* You can track your training using **wandb**\n",
    "* It's very easy to compare model's performance using **wandb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update\n",
    "- `v14`: vertical flip augmentation\n",
    "- `v9`: use seperate heads for each organ (total $5$ heads now, $2$ heads with `sigmoid`, $3$ with `softmax`.\n",
    "- `v7`: reduce class variables using class correlation\n",
    "- `v4`: monitor `val_acc`, max_lr reduced, model: effnetb4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "D:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # to avoid too many logging messages\n",
    "import pandas as pd, numpy as np, random, shutil\n",
    "import tensorflow as tf, re, math\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from IPython import display as ipd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np: 1.26.0\n",
      "pd: 2.1.3\n",
      "sklearn: 1.3.2\n",
      "tf: 2.10.0\n",
      "tfp: 0.14.0\n",
      "tfa: 0.22.0\n",
      "w&b: 0.16.0\n"
     ]
    }
   ],
   "source": [
    "print('np:', np.__version__)\n",
    "print('pd:', pd.__version__)\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('tf:',tf.__version__)\n",
    "print('tfp:', tfp.__version__)\n",
    "print('tfa:', tfa.__version__)\n",
    "print('w&b:', wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management\n",
    "\n",
    "Some of the cool features of **W&B**:\n",
    "\n",
    "* Track, compare, and visualize ML experiments\n",
    "* Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.\n",
    "* Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.\n",
    "\n",
    "> **Note:** `kaggle_secrets` has not been added to `TPU-VM` yet, so run anonymously then go to the dashbaord and simply `claim` the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use your W&B account,\n",
      "Go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"WANDB\")\n",
    "\n",
    "    wandb.login(key=api_key)\n",
    "    anonymous = None\n",
    "except:\n",
    "    anonymous = \"must\"\n",
    "    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb         = True\n",
    "    competition   = 'rsna-atd' \n",
    "    _wandb_kernel = 'awsaf49'\n",
    "    debug         = False\n",
    "    comment       = 'EfficientNetV1B0-256x256-low_lr-vflip'\n",
    "    exp_name      = 'baseline-v4: new_ds + multi_head' # name of the experiment, folds will be grouped using 'exp_name'\n",
    "    \n",
    "    # use verbose=0 for silent, vebose=1 for interactive,\n",
    "    verbose      = 0\n",
    "    display_plot = True\n",
    "\n",
    "    # device\n",
    "    device = \"TPU-VM\" #or \"GPU\"\n",
    "\n",
    "    model_name = 'EfficientNetV1B0'\n",
    "\n",
    "    # seed for data-split, layer init, augs\n",
    "    seed = 42\n",
    "\n",
    "    # number of folds for data-split\n",
    "    folds = 4\n",
    "    \n",
    "    # which folds to train\n",
    "    selected_folds = [0, 1, 2]\n",
    "\n",
    "    # size of the image\n",
    "    img_size = [256, 256]\n",
    "#     eq_dim = np.prod(img_size)**0.5\n",
    "\n",
    "    # batch_size and epochs\n",
    "    batch_size = 48\n",
    "    epochs = 10\n",
    "\n",
    "    # loss\n",
    "    loss      = 'BCE & CCE'  # BCE, Focal\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = 'Adam'\n",
    "\n",
    "    # augmentation\n",
    "    augment   = True\n",
    "\n",
    "    # scale-shift-rotate-shear\n",
    "    transform = 0.90  # transform prob\n",
    "    fill_mode = 'constant'\n",
    "    rot    = 2.0\n",
    "    shr    = 2.0\n",
    "    hzoom  = 50.0\n",
    "    wzoom  = 50.0\n",
    "    hshift = 10.0\n",
    "    wshift = 10.0\n",
    "\n",
    "    # flip\n",
    "    hflip = True\n",
    "    vflip = True\n",
    "\n",
    "    # clip\n",
    "    clip = False\n",
    "\n",
    "    # lr-scheduler\n",
    "    scheduler   = 'cosine' # cosine\n",
    "\n",
    "    # dropout\n",
    "    drop_prob   = 0.6\n",
    "    drop_cnt    = 5\n",
    "    drop_size   = 0.05\n",
    "    \n",
    "    # cut-mix-up\n",
    "    mixup_prob = 0.0\n",
    "    mixup_alpha = 0.5\n",
    "    \n",
    "    cutmix_prob = 0.0\n",
    "    cutmix_alpha = 2.5\n",
    "\n",
    "    # pixel-augment\n",
    "    pixel_aug = 0.90  # prob of pixel_aug\n",
    "    sat  = [0.7, 1.3]\n",
    "    cont = [0.8, 1.2]\n",
    "    bri  = 0.15\n",
    "    hue  = 0.05\n",
    "\n",
    "    # test-time augs\n",
    "    tta = 1\n",
    "    \n",
    "    # target column\n",
    "    target_col  = [ \"bowel_injury\", \"extravasation_injury\", \"kidney_healthy\", \"kidney_low\",\n",
    "                   \"kidney_high\", \"liver_healthy\", \"liver_low\", \"liver_high\",\n",
    "                   \"spleen_healthy\", \"spleen_low\", \"spleen_high\"] # not using \"bowel_healthy\" & \"extravasation_healthy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeding done!!!\n"
     ]
    }
   ],
   "source": [
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "#     os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    print('seeding done!!!')\n",
    "seeding(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Configs\n",
    "This notebook is compatible for **remote-tpu**, **local-tpu**, **multi-gpu** and **single-gpu**. Simple change to `device=\"TPU\"` for **remote-tpu** and `device=\"TPU-VM\"` for **local-tpu** and finally, `device=\"GPU\"` for single or multi-gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "Using CPU\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "if \"TPU\" in CFG.device:\n",
    "    tpu = 'local' if CFG.device=='TPU-VM' else None\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    except:\n",
    "        CFG.device = \"GPU\"\n",
    "        \n",
    "if CFG.device == \"GPU\"  or CFG.device==\"CPU\":\n",
    "    ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    if ngpu>1:\n",
    "        print(\"Using multi GPU\")\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    elif ngpu==1:\n",
    "        print(\"Using single GPU\")\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        CFG.device = \"CPU\"\n",
    "\n",
    "if CFG.device == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", ngpu)\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROOT PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# define the path to my data\n",
    "BASE_PATH = 's:/Capston/data2'\n",
    "STRIDE = 10 # take one patient after each (STRIDE - 1) patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Data\n",
    "\n",
    "The `train.csv` file contains the following meta information:\n",
    "\n",
    "- `patient_id`: A unique ID code for each patient.\n",
    "- `series_id`: A unique ID code for each scan.\n",
    "- `instance_number`: The image number within the scan. The lowest instance number for many series is above zero as the original scans were cropped to the abdomen.\n",
    "- `[bowel/extravasation]_[healthy/injury]`: The two injury types with binary targets.\n",
    "- `[kidney/liver/spleen]_[healthy/low/high]`: The three injury types with three target levels.\n",
    "- `any_injury`: Whether the patient had any injury at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'series_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5668\\3114499493.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{BASE_PATH}/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_path'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{BASE_PATH}/train_images'\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatient_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstance_number\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'series_id'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "df['image_path'] = f'{BASE_PATH}/train_images'\\\n",
    "                    + '/' + df.patient_id.astype(str)\\\n",
    "                    + '/' + df.series_id.astype(str)\\\n",
    "                    + '/' + df.instance_number.astype(str) +'.png'\n",
    "df = df.drop_duplicates()\n",
    "print('Train:')\n",
    "display(df.head(2))\n",
    "\n",
    "# test\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "test_df['image_path'] = f'{BASE_PATH}/test_images'\\\n",
    "                    + '/' + test_df.patient_id.astype(str)\\\n",
    "                    + '/' + test_df.series_id.astype(str)\\\n",
    "                    + '/' + test_df.instance_number.astype(str) +'.png'\n",
    "test_df = test_df.drop_duplicates()\n",
    "print('\\nTest:')\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_id', 'bowel_healthy', 'bowel_injury', 'extravasation_healthy',\n",
       "       'extravasation_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
       "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
       "       'spleen_low', 'spleen_high', 'any_injury'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check If Data Exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:15.883987Z",
     "iopub.status.busy": "2023-07-28T05:21:15.882007Z",
     "iopub.status.idle": "2023-07-28T05:21:15.903858Z",
     "shell.execute_reply": "2023-07-28T05:21:15.902874Z",
     "shell.execute_reply.started": "2023-07-28T05:21:15.883952Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.io.gfile.exists(df.image_path.iloc[0]), tf.io.gfile.exists(test_df.image_path.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Ditribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:15.905785Z",
     "iopub.status.busy": "2023-07-28T05:21:15.905183Z",
     "iopub.status.idle": "2023-07-28T05:21:15.910943Z",
     "shell.execute_reply": "2023-07-28T05:21:15.909981Z",
     "shell.execute_reply.started": "2023-07-28T05:21:15.905749Z"
    }
   },
   "outputs": [],
   "source": [
    "print('train_files:',df.shape[0])\n",
    "print('test_files:',test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Dependency\n",
    "\n",
    "In the context of our analysis, class dependencies refer to inherent relationships between different classes. For instance, the status of a scan cannot be simultaneously classified as `injured` and `healthy`. This correlation becomes evident when we visualize the data through a heatmap. It's worth noting that identifying and understanding these relationships allow us to simplify our model by removing redundant information.\n",
    "\n",
    "## Dependencies Between `injury` and `healthy` in `bowel` and`extravasation`\n",
    "\n",
    "A key observation from our data visualizations is the perfect complementarity between `bowel_injury` and `bowel_healthy`, as well as between `extravasation_injury` and `extravasation_healthy`. This essentially means that the sum of each pair will always equal `1.0`. \n",
    "\n",
    "For the purpose of our model, we will leverage this relationship by only including `{bowel/extravasation}_injury` as variables. We can then calculate the complementary `healthy` status by applying a `sigmoid` function to the corresponding `injury` status.\n",
    "\n",
    "## Dependencies Between `healthy`, `low`, and `high` in `kidney`, `liver`, and `spleen` \n",
    "\n",
    "Another notable relationship in our data pertains to the `{kidney/liver/spleen}_[healthy/low/high]` classifications. These variables are \"softmaxed,\" meanining that the combined probabilities of `healthy`, `low`, and `high` for each organ (`kidney`, `liver`, and `spleen`) sum up to `1.0`.  \n",
    "\n",
    "In our model, we will apply the `softmax` function to these variables, effectively reducing the complexity of the model while preserving the essential information for each classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:15.913028Z",
     "iopub.status.busy": "2023-07-28T05:21:15.912248Z",
     "iopub.status.idle": "2023-07-28T05:21:16.938075Z",
     "shell.execute_reply": "2023-07-28T05:21:16.937173Z",
     "shell.execute_reply.started": "2023-07-28T05:21:15.912995Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df[df.columns[1:14]].corr().round(2)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "* Data is splited while stratifying, 'laterality', 'view', 'age','cancer', 'biopsy', 'invasive', 'BIRADS', 'implant', 'density','machine_id', 'difficult_negative_case','cancer'.\n",
    "* To avoid leakage, data is also split keeping images from same patient in either train or valid not in both.\n",
    "* `StratifiedGroupKFold` does the both job **stratifying** and **group** split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:16.939427Z",
     "iopub.status.busy": "2023-07-28T05:21:16.939113Z",
     "iopub.status.idle": "2023-07-28T05:21:17.214536Z",
     "shell.execute_reply": "2023-07-28T05:21:17.21364Z",
     "shell.execute_reply.started": "2023-07-28T05:21:16.939397Z"
    }
   },
   "outputs": [],
   "source": [
    "df['stratify'] = ''\n",
    "for col in CFG.target_col:\n",
    "    df['stratify'] += df[col].astype(str)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "skf = StratifiedGroupKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['stratify'], df[\"patient_id\"])):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "display(df.groupby(['fold', 'patient_id']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Used simple augmentations, some of them may hurt the model.\n",
    "* RandomFlip (Left-Right)\n",
    "* No Rotation\n",
    "* RandomBrightness\n",
    "* RndomContrast\n",
    "* Shear\n",
    "* Zoom\n",
    "* Coarsee Dropout/Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:17.216874Z",
     "iopub.status.busy": "2023-07-28T05:21:17.216166Z",
     "iopub.status.idle": "2023-07-28T05:21:17.241412Z",
     "shell.execute_reply": "2023-07-28T05:21:17.240416Z",
     "shell.execute_reply.started": "2023-07-28T05:21:17.21684Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    #rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "#     c1   = tf.math.cos(rotation)\n",
    "#     s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "#     rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "#                                    -s1,  c1,   zero, \n",
    "#                                    zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                               zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "\n",
    "    return  K.dot(shear_matrix,K.dot(zoom_matrix, shift_matrix)) #K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))                  \n",
    "\n",
    "def transform(image, DIM=CFG.img_size):#[rot,shr,h_zoom,w_zoom,h_shift,w_shift]):\n",
    "    if DIM[0]>DIM[1]:\n",
    "        diff  = (DIM[0]-DIM[1])\n",
    "        pad   = [diff//2, diff//2 + diff%2]\n",
    "        image = tf.pad(image, [[0, 0], [pad[0], pad[1]],[0, 0]])\n",
    "        NEW_DIM = DIM[0]\n",
    "    elif DIM[0]<DIM[1]:\n",
    "        diff  = (DIM[1]-DIM[0])\n",
    "        pad   = [diff//2, diff//2 + diff%2]\n",
    "        image = tf.pad(image, [[pad[0], pad[1]], [0, 0],[0, 0]])\n",
    "        NEW_DIM = DIM[1]\n",
    "    \n",
    "    rot = CFG.rot * tf.random.normal([1], dtype='float32')\n",
    "    shr = CFG.shr * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.hzoom\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.wzoom\n",
    "    h_shift = CFG.hshift * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = CFG.wshift * tf.random.normal([1], dtype='float32') \n",
    "    \n",
    "    transformation_matrix=tf.linalg.inv(get_mat(shr,h_zoom,w_zoom,h_shift,w_shift))\n",
    "    \n",
    "    flat_tensor=tfa.image.transform_ops.matrices_to_flat_transforms(transformation_matrix)\n",
    "    \n",
    "    image=tfa.image.transform(image,flat_tensor, fill_mode=CFG.fill_mode)\n",
    "    \n",
    "    rotation = math.pi * rot / 180.\n",
    "    \n",
    "    image=tfa.image.rotate(image,-rotation, fill_mode=CFG.fill_mode)\n",
    "    \n",
    "    if DIM[0]>DIM[1]:\n",
    "        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n",
    "        image = image[:, pad[0]:-pad[1],:]\n",
    "    elif DIM[1]>DIM[0]:\n",
    "        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n",
    "        image = image[pad[0]:-pad[1],:,:]\n",
    "    image = tf.reshape(image, [*DIM, 3])    \n",
    "    return image\n",
    "\n",
    "def dropout(image,DIM=CFG.img_size, PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "    \n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n",
    "    if (P==0)|(CT==0)|(SZ==0): \n",
    "        return image\n",
    "    \n",
    "    for k in range(CT):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM[0],y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM[1],x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n",
    "        three = image[ya:yb,xb:DIM[1],:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0)\n",
    "        image = tf.reshape(image,[*DIM,3])\n",
    "\n",
    "#     image = tf.reshape(image,[*DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:17.243508Z",
     "iopub.status.busy": "2023-07-28T05:21:17.24313Z",
     "iopub.status.idle": "2023-07-28T05:21:17.265099Z",
     "shell.execute_reply": "2023-07-28T05:21:17.264062Z",
     "shell.execute_reply.started": "2023-07-28T05:21:17.243476Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_int(shape=[], minval=0, maxval=1):\n",
    "    return tf.random.uniform(\n",
    "        shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n",
    "\n",
    "\n",
    "def random_float(shape=[], minval=0.0, maxval=1.0):\n",
    "    rnd = tf.random.uniform(\n",
    "        shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n",
    "    return rnd\n",
    "\n",
    "# mixup\n",
    "def get_mixup(alpha=0.2, prob=0.5):\n",
    "    @tf.function\n",
    "    def mixup(images, labels, alpha=alpha, prob=prob):\n",
    "        if random_float() > prob:\n",
    "            return images, labels\n",
    "\n",
    "        image_shape = tf.shape(images)\n",
    "        label_shape = tf.shape(labels)\n",
    "\n",
    "        beta = tfp.distributions.Beta(alpha, alpha)\n",
    "        lam = beta.sample(1)[0]\n",
    "\n",
    "        images = lam * images + (1.0 - lam) * tf.roll(images, shift=1, axis=0)\n",
    "        labels = lam * labels + (1.0 - lam) * tf.roll(labels, shift=1, axis=0)\n",
    "\n",
    "        images = tf.reshape(images, image_shape)\n",
    "        labels = tf.reshape(labels, label_shape)\n",
    "        return images, labels\n",
    "    return mixup\n",
    "\n",
    "# cutmix\n",
    "def get_cutmix(alpha, prob=0.5):\n",
    "    @tf.function\n",
    "    def cutmix(images, labels, alpha=alpha, prob=prob):\n",
    "        if random_float() > prob:\n",
    "            return images, labels\n",
    "        image_shape = tf.shape(images)\n",
    "        label_shape = tf.shape(labels)\n",
    "        \n",
    "        W = tf.cast(image_shape[2], tf.int32)\n",
    "        H = tf.cast(image_shape[1], tf.int32)\n",
    "\n",
    "        beta = tfp.distributions.Beta(alpha, alpha)\n",
    "        lam = beta.sample(1)[0]\n",
    "\n",
    "        images_rolled = tf.roll(images, shift=1, axis=0)\n",
    "        labels_rolled = tf.roll(labels, shift=1, axis=0)\n",
    "\n",
    "        r_x = random_int([], minval=0, maxval=W)\n",
    "        r_y = random_int([], minval=0, maxval=H)\n",
    "        r = 0.5 * tf.math.sqrt(1.0 - lam)\n",
    "        r_w_half = tf.cast(r * tf.cast(W, tf.float32), tf.int32)\n",
    "        r_h_half = tf.cast(r * tf.cast(H, tf.float32), tf.int32)\n",
    "\n",
    "        x1 = tf.cast(tf.clip_by_value(r_x - r_w_half, 0, W), tf.int32)\n",
    "        x2 = tf.cast(tf.clip_by_value(r_x + r_w_half, 0, W), tf.int32)\n",
    "        y1 = tf.cast(tf.clip_by_value(r_y - r_h_half, 0, H), tf.int32)\n",
    "        y2 = tf.cast(tf.clip_by_value(r_y + r_h_half, 0, H), tf.int32)\n",
    "\n",
    "        # outer-pad patch -> [0, 0, 1, 1, 0, 0]\n",
    "        patch1 = images[:, y1:y2, x1:x2, :]  # [batch, height, width, channel]\n",
    "        patch1 = tf.pad(\n",
    "            patch1, [[0, 0], [y1, H - y2], [x1, W - x2], [0, 0]])  # outer-pad\n",
    "\n",
    "        # inner-pad patch -> [1, 1, 0, 0, 1, 1]\n",
    "        patch2 = images_rolled[:, y1:y2, x1:x2, :]\n",
    "        patch2 = tf.pad(\n",
    "            patch2, [[0, 0], [y1, H - y2], [x1, W - x2], [0, 0]])  # outer-pad\n",
    "        patch2 = images_rolled - patch2  # inner-pad = img - outer-pad\n",
    "\n",
    "        images = patch1 + patch2  # cutmix img\n",
    "\n",
    "        lam = tf.cast((1.0 - (x2 - x1) * (y2 - y1) / (W * H)), tf.float32)  # no H as (y1 - y2)/H = 1\n",
    "        labels = lam * labels + (1.0 - lam) * labels_rolled  # cutmix label\n",
    "\n",
    "        images = tf.reshape(images, image_shape)\n",
    "        labels = tf.reshape(labels, label_shape)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    return cutmix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "* Reads the raw file and then decodes it to tf.Tensor\n",
    "* Resizes the image in desired size\n",
    "* Chages the datatype to **float32**\n",
    "* Caches the Data for boosting up the speed.\n",
    "* Uses Augmentations to reduce overfitting and make model more robust.\n",
    "* Finally, splits the data into batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:17.267023Z",
     "iopub.status.busy": "2023-07-28T05:21:17.266651Z",
     "iopub.status.idle": "2023-07-28T05:21:17.289201Z",
     "shell.execute_reply": "2023-07-28T05:21:17.288078Z",
     "shell.execute_reply.started": "2023-07-28T05:21:17.26699Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_decoder(with_labels=True, target_size=CFG.img_size, ext='png'):\n",
    "    def decode_image(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3, dtype=tf.uint8)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "        img = tf.image.resize(img, target_size, method='bilinear')\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.reshape(img, [*target_size, 3])\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_label(label):\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        return (label[0:1], label[1:2], label[2:5], label[5:8], label[8:11])\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode_image(path), decode_label(label)\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_augmenter(with_labels=True, dim=CFG.img_size):\n",
    "    def augment(img, dim=dim):\n",
    "        if random_float() < CFG.transform:\n",
    "            img = transform(img,DIM=dim)\n",
    "        img = tf.image.random_flip_left_right(img) if CFG.hflip else img\n",
    "        img = tf.image.random_flip_up_down(img) if CFG.vflip else img\n",
    "        if random_float() < CFG.pixel_aug:\n",
    "            img = tf.image.random_hue(img, CFG.hue)\n",
    "            img = tf.image.random_saturation(img, CFG.sat[0], CFG.sat[1])\n",
    "            img = tf.image.random_contrast(img, CFG.cont[0], CFG.cont[1])\n",
    "            img = tf.image.random_brightness(img, CFG.bri)\n",
    "        img = tf.clip_by_value(img, 0, 1)  if CFG.clip else img         \n",
    "        img = tf.reshape(img, [*dim, 3])\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):    \n",
    "        return augment(img), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment\n",
    "\n",
    "\n",
    "def build_dataset(paths, labels=None, batch_size=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=True, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\", drop_remainder=False):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    if augment and labels is not None:\n",
    "        ds = ds.map(lambda img, label: (dropout(img, \n",
    "                                               DIM=CFG.img_size, \n",
    "                                               PROBABILITY=CFG.drop_prob, \n",
    "                                               CT=CFG.drop_cnt,\n",
    "                                               SZ=CFG.drop_size), label),num_parallel_calls=AUTO)\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    if augment and labels is not None:\n",
    "        if CFG.cutmix_prob:\n",
    "            ds = ds.map(get_cutmix(alpha=CFG.cutmix_alpha,prob=CFG.cutmix_prob),num_parallel_calls=AUTO)\n",
    "        if CFG.mixup_prob:\n",
    "            ds = ds.map(get_mixup(alpha=CFG.mixup_alpha,prob=CFG.mixup_prob),num_parallel_calls=AUTO)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "* Check if augmentation is working properly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:17.290965Z",
     "iopub.status.busy": "2023-07-28T05:21:17.290644Z",
     "iopub.status.idle": "2023-07-28T05:21:17.306764Z",
     "shell.execute_reply": "2023-07-28T05:21:17.305825Z",
     "shell.execute_reply.started": "2023-07-28T05:21:17.290933Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_batch(batch, size=2):\n",
    "    if isinstance(batch, tuple):\n",
    "        imgs, tars = batch\n",
    "    else:\n",
    "        imgs = batch\n",
    "        tars = None\n",
    "    tars = tf.concat(tars,axis=-1).numpy()\n",
    "    plt.figure(figsize=(size*5, 10))\n",
    "    for img_idx in range(size):\n",
    "        plt.subplot(1, size, img_idx+1)\n",
    "        if tars is not None:\n",
    "            plt.title(f'{tars[img_idx].round(2)}', fontsize=12)\n",
    "        img = imgs[img_idx,]\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:17.308271Z",
     "iopub.status.busy": "2023-07-28T05:21:17.307879Z",
     "iopub.status.idle": "2023-07-28T05:21:18.628594Z",
     "shell.execute_reply": "2023-07-28T05:21:18.627596Z",
     "shell.execute_reply.started": "2023-07-28T05:21:17.30824Z"
    }
   },
   "outputs": [],
   "source": [
    "fold = 0\n",
    "fold_df = df[df.fold==fold].sample(frac=1.0)\n",
    "paths  = fold_df.image_path.tolist()\n",
    "labels = fold_df[CFG.target_col].values\n",
    "ds = build_dataset(paths, labels, cache=False, batch_size=32,\n",
    "                   repeat=True, shuffle=True, augment=False)\n",
    "ds = ds.unbatch().batch(20)\n",
    "batch = next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:18.632478Z",
     "iopub.status.busy": "2023-07-28T05:21:18.632172Z",
     "iopub.status.idle": "2023-07-28T05:21:19.39348Z",
     "shell.execute_reply": "2023-07-28T05:21:19.392676Z",
     "shell.execute_reply.started": "2023-07-28T05:21:18.632453Z"
    }
   },
   "outputs": [],
   "source": [
    "display_batch(batch, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:19.395736Z",
     "iopub.status.busy": "2023-07-28T05:21:19.394767Z",
     "iopub.status.idle": "2023-07-28T05:21:19.401759Z",
     "shell.execute_reply": "2023-07-28T05:21:19.399075Z",
     "shell.execute_reply.started": "2023-07-28T05:21:19.395702Z"
    }
   },
   "outputs": [],
   "source": [
    "# mixup = get_mixup(alpha=2.5, prob=1)\n",
    "# mimgs, mtars = mixup(batch[0], batch[1])\n",
    "# display_batch((mimgs, mtars), 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:19.404979Z",
     "iopub.status.busy": "2023-07-28T05:21:19.404617Z",
     "iopub.status.idle": "2023-07-28T05:21:19.52861Z",
     "shell.execute_reply": "2023-07-28T05:21:19.527396Z",
     "shell.execute_reply.started": "2023-07-28T05:21:19.404944Z"
    }
   },
   "outputs": [],
   "source": [
    "# cutmix = get_cutmix(alpha=2.5, prob=1)\n",
    "# cimgs, ctars = cutmix(batch[0], batch[1])\n",
    "# display_batch((cimgs, ctars), 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:19.530225Z",
     "iopub.status.busy": "2023-07-28T05:21:19.529935Z",
     "iopub.status.idle": "2023-07-28T05:21:22.086673Z",
     "shell.execute_reply": "2023-07-28T05:21:22.085793Z",
     "shell.execute_reply.started": "2023-07-28T05:21:19.530201Z"
    }
   },
   "outputs": [],
   "source": [
    "dimgs = tf.map_fn(lambda img: dropout(img,\n",
    "                DIM=CFG.img_size, \n",
    "                PROBABILITY=1.0, \n",
    "                CT=10,\n",
    "                SZ=0.08), batch[0])\n",
    "dtars = batch[1]\n",
    "display_batch((dimgs, dtars), 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:22.08895Z",
     "iopub.status.busy": "2023-07-28T05:21:22.088242Z",
     "iopub.status.idle": "2023-07-28T05:21:27.970269Z",
     "shell.execute_reply": "2023-07-28T05:21:27.969422Z",
     "shell.execute_reply.started": "2023-07-28T05:21:22.088915Z"
    }
   },
   "outputs": [],
   "source": [
    "timgs = tf.map_fn(lambda img: transform(img,DIM=CFG.img_size), batch[0])\n",
    "ttars = batch[1]\n",
    "display_batch((timgs, ttars), 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "We'll use Binary Cross Entropy loss for `bowel` and `extravasation` at the same time we'll use Categorical Cross Entropy for `kidney`, `liver` and `spleen`.\n",
    "<!-- ## BCE\n",
    "Loss Function for this notebook is **BCE: Binary Crossentropy** or **Focal** as the task is **binary classification**\n",
    "\n",
    "$$\\textrm{BCE}  = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\cdot log(\\hat{y}_i) + (1 - y_i) \\cdot log(1 - \\hat{y}_i)$$\n",
    "\n",
    "where $\\hat{y}_i$ is the **predicted** value and $y_i$ is the **original** value for each instance $i$.\n",
    "\n",
    "## Focal\n",
    "$$\\textrm{FL} = -\\alpha_{t}(1 - p_{t})^{\\gamma}\\log{p_{t}}$$\n",
    "\n",
    "$\\gamma$ controls the shape of the curve. The higher the value of $\\gamma$, the lower the loss for well-classified examples, so we could turn the attention of the model more towards ‘hard-to-classify examples. FL gives high weights to the rare class and small weights to the dominating or common class. These weights are referred to as $\\alpha$.\n",
    "\n",
    "\n",
    "## Code\n",
    "* `tf.keras.losses.BinaryCrossentropy`\n",
    "* `tfa.losses.SigmoidFocalCrossEntropy` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:27.972523Z",
     "iopub.status.busy": "2023-07-28T05:21:27.972181Z",
     "iopub.status.idle": "2023-07-28T05:21:28.040056Z",
     "shell.execute_reply": "2023-07-28T05:21:28.039131Z",
     "shell.execute_reply.started": "2023-07-28T05:21:27.972491Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras_cv_attention_models import efficientnet\n",
    "\n",
    "def build_model(model_name=CFG.model_name,\n",
    "                loss_name=CFG.loss,\n",
    "                dim=CFG.img_size,\n",
    "                compile_model=True,\n",
    "                include_top=False):         \n",
    "    \n",
    "    # Define backbone\n",
    "    base = getattr(efficientnet, model_name)(input_shape=(*dim,3),\n",
    "                                    pretrained='imagenet',\n",
    "                                    num_classes=0) # get base model (efficientnet), use imgnet weights\n",
    "\n",
    "    inp = base.inputs\n",
    "    x = base.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) # use GAP to get pooling result form conv outputs\n",
    "\n",
    "    # Define 'necks' for each head\n",
    "    x_bowel = tf.keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_extra = tf.keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_liver = tf.keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_kidney = tf.keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_spleen = tf.keras.layers.Dense(32, activation='silu')(x)\n",
    "\n",
    "    # Define heads\n",
    "    out_bowel = tf.keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n",
    "    out_extra = tf.keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n",
    "    out_liver = tf.keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n",
    "    out_kidney = tf.keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n",
    "    out_spleen = tf.keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n",
    "\n",
    "    # Combine outputs\n",
    "#     out = tf.keras.layers.Concatenate()([out_bowel, out_extra, \n",
    "#                                          out_liver, out_kidney, out_spleen])\n",
    "    out = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    \n",
    "    if compile_model:\n",
    "        # optimizer\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        # loss\n",
    "        loss = {\n",
    "            'bowel':tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),\n",
    "            'extra':tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),\n",
    "            'liver':tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "            'kidney':tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "            'spleen':tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "        }\n",
    "        # metric\n",
    "        metrics = {\n",
    "            'bowel':['accuracy'],\n",
    "            'extra':['accuracy'],\n",
    "            'liver':['accuracy'],\n",
    "            'kidney':['accuracy'],\n",
    "            'spleen':['accuracy'],\n",
    "        }\n",
    "        # compile\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=loss,\n",
    "                      metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:28.042037Z",
     "iopub.status.busy": "2023-07-28T05:21:28.04169Z",
     "iopub.status.idle": "2023-07-28T05:21:32.123512Z",
     "shell.execute_reply": "2023-07-28T05:21:32.122493Z",
     "shell.execute_reply.started": "2023-07-28T05:21:28.042004Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = build_model(CFG.model_name, dim=CFG.img_size, compile_model=True)\n",
    "# tmp.summary()  # too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning-Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:32.131044Z",
     "iopub.status.busy": "2023-07-28T05:21:32.128607Z",
     "iopub.status.idle": "2023-07-28T05:21:32.590402Z",
     "shell.execute_reply": "2023-07-28T05:21:32.589522Z",
     "shell.execute_reply.started": "2023-07-28T05:21:32.131007Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8, plot=False):\n",
    "    lr_start   = 0.000005\n",
    "    lr_max     = 0.00000050 * REPLICAS * batch_size\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 4\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        elif CFG.scheduler=='exp':\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        elif CFG.scheduler=='cosine':\n",
    "            decay_total_epochs = CFG.epochs - lr_ramp_ep - lr_sus_ep + 3\n",
    "            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            cosine_decay = 0.4 * (1 + math.cos(phase))\n",
    "            lr = (lr_max - lr_min) * cosine_decay + lr_min\n",
    "        return lr\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(np.arange(CFG.epochs), [lrfn(epoch) for epoch in np.arange(CFG.epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n",
    "        plt.title('Learning Rate Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "_=get_lr_callback(CFG.batch_size, plot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Wandb** Logger\n",
    "Log:\n",
    "* Best Score\n",
    "* Attention MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:32.597022Z",
     "iopub.status.busy": "2023-07-28T05:21:32.594761Z",
     "iopub.status.idle": "2023-07-28T05:21:33.92117Z",
     "shell.execute_reply": "2023-07-28T05:21:33.919731Z",
     "shell.execute_reply.started": "2023-07-28T05:21:32.596985Z"
    }
   },
   "outputs": [],
   "source": [
    "# create directory to save gradcam imgs\n",
    "!mkdir -p gradcam\n",
    "\n",
    "# intialize wandb run\n",
    "def wandb_init(fold):\n",
    "    config = {k:v for k,v in dict(vars(CFG)).items() if '__' not in k}\n",
    "    config.update({\"fold\":int(fold)})\n",
    "    yaml.dump(config, open(f'/kaggle/working/config fold-{fold}.yaml', 'w'),)\n",
    "    config = yaml.load(open(f'/kaggle/working/config fold-{fold}.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "    run    = wandb.init(project=\"rsna-atd-public\",\n",
    "               name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}\",\n",
    "               config=config,\n",
    "               anonymous=anonymous,\n",
    "               group=CFG.exp_name\n",
    "                    )\n",
    "    return run\n",
    "\n",
    "def log_wandb(fold):\n",
    "    \"log best result for error analysis\"\n",
    "    # log values to wandb\n",
    "    wandb.log({\n",
    "               'best_acc': best_acc,\n",
    "               'best_loss': best_loss,\n",
    "               'best_epoch': best_epoch,\n",
    "               'best_acc_bowel': best_acc_bowel,\n",
    "               'best_acc_extra': best_acc_extra,\n",
    "               'best_acc_liver': best_acc_liver,\n",
    "               'best_acc_kidney': best_acc_kidney,\n",
    "               'best_acc_spleen': best_acc_spleen,\n",
    "              })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `WandbModelCheckpoint` and `WandbMetricsLogger`\n",
    "Newly released callbacks offers more flexibility in terms of customization and also they are compact comparing classic `WandbCallback` hence it easier to use. Here's a brief intro about them,\n",
    "\n",
    "* **WandbModelCheckpoint**: This callback saves model or weights using `tf.keras.callbacks.ModelCheckpoint`, hence we can harness the power of official tf callback to do so much more such log `tf.keras.Model` subclass model in TPU. \n",
    "\n",
    "* **WandbMetricsLogger**: This callback simply logs all the metrics and losses.\n",
    "\n",
    "* **WandbEvalCallback**: This one is even more special, we can use it to log model's prediction after certain epoch/frequency. We can use it to save segmentation mask, bounding boxes and gradcam within epochs to check intermediate result.\n",
    "\n",
    "For more details, check [here](https://docs.wandb.ai/ref/python/integrations/keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:33.934601Z",
     "iopub.status.busy": "2023-07-28T05:21:33.932332Z",
     "iopub.status.idle": "2023-07-28T05:21:33.943127Z",
     "shell.execute_reply": "2023-07-28T05:21:33.942054Z",
     "shell.execute_reply.started": "2023-07-28T05:21:33.934563Z"
    }
   },
   "outputs": [],
   "source": [
    "# get wandb callbacks\n",
    "def get_wb_callbacks(fold):\n",
    "    wb_ckpt = wandb.keras.WandbModelCheckpoint(filepath='fold-%i.h5'%fold, \n",
    "                                               monitor='val_loss',\n",
    "                                               verbose=CFG.verbose,\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=False,\n",
    "                                               mode='min',)\n",
    "    wb_metr = wandb.keras.WandbMetricsLogger()\n",
    "    return [wb_ckpt, wb_metr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "* Cross-Validation: 5 fold\n",
    "* **WandB** dashboard is shown end of the each fold. So we don't need to plot anything. We can select best model from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:21:33.950109Z",
     "iopub.status.busy": "2023-07-28T05:21:33.947567Z",
     "iopub.status.idle": "2023-07-28T05:24:29.217967Z",
     "shell.execute_reply": "2023-07-28T05:24:29.216529Z",
     "shell.execute_reply.started": "2023-07-28T05:21:33.950077Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for fold in np.arange(CFG.folds):\n",
    "    \n",
    "    # ignore not selected folds\n",
    "    if fold not in CFG.selected_folds:\n",
    "        continue\n",
    "        \n",
    "    # init wandb\n",
    "    if CFG.wandb:\n",
    "        run = wandb_init(fold)\n",
    "        wb_callbacks = get_wb_callbacks(fold)\n",
    "            \n",
    "    # train and valid dataframe\n",
    "    train_df = df.query(\"fold!=@fold\")\n",
    "    valid_df = df.query(\"fold==@fold\")\n",
    "    \n",
    "    # get image_paths and labels\n",
    "    train_paths = train_df.image_path.values; train_labels = train_df[CFG.target_col].values.astype(np.float32)\n",
    "    valid_paths = valid_df.image_path.values; valid_labels = valid_df[CFG.target_col].values.astype(np.float32)\n",
    "    test_paths  = test_df.image_path.values\n",
    "    \n",
    "    # shuffle train data\n",
    "    index = np.arange(len(train_df))\n",
    "    np.random.shuffle(index)\n",
    "    train_paths  = train_paths[index]\n",
    "    train_labels = train_labels[index]\n",
    "    \n",
    "    # min samples in debug mode\n",
    "    min_samples = CFG.batch_size*REPLICAS*2\n",
    "    \n",
    "    # for debug model run on small portion\n",
    "    if CFG.debug:\n",
    "        train_paths = train_paths[:min_samples]; train_labels = train_labels[:min_samples]\n",
    "        valid_paths = valid_paths[:min_samples]; valid_labels = valid_labels[:min_samples]\n",
    "    \n",
    "    # show message\n",
    "    print('#'*40); print('#### FOLD: ',fold)\n",
    "    print('#### IMAGE_SIZE: (%i, %i) | MODEL_NAME: %s | BATCH_SIZE: %i'%\n",
    "          (CFG.img_size[0],CFG.img_size[1],CFG.model_name,CFG.batch_size*REPLICAS))\n",
    "    \n",
    "    # data stat\n",
    "    num_train = len(train_paths)\n",
    "    num_valid = len(valid_paths)\n",
    "    if CFG.wandb:\n",
    "        wandb.log({'num_train':num_train,\n",
    "                   'num_valid':num_valid})\n",
    "    print('#### NUM_TRAIN: {:,} | NUM_VALID: {:,}'.format(num_train, num_valid))\n",
    "    \n",
    "    # build model\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(CFG.model_name, dim=CFG.img_size, compile_model=True)\n",
    "\n",
    "    # build dataset\n",
    "    cache = 1 if 'TPU' in CFG.device else 0\n",
    "    train_ds = build_dataset(train_paths, train_labels, cache=cache, batch_size=CFG.batch_size*REPLICAS,\n",
    "                   repeat=True, shuffle=True, augment=CFG.augment)\n",
    "    val_ds = build_dataset(valid_paths, valid_labels, cache=cache, batch_size=CFG.batch_size*REPLICAS,\n",
    "                   repeat=False, shuffle=False, augment=False)\n",
    "    print('#'*40)   \n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    ## save best model after each fold\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        'fold-%i.h5'%fold, monitor='val_loss', verbose=CFG.verbose, save_best_only=True,\n",
    "        save_weights_only=False, mode='min', save_freq='epoch')\n",
    "    callbacks +=[sv]\n",
    "    ## lr-scheduler\n",
    "    callbacks += [get_lr_callback(CFG.batch_size)]\n",
    "    ## wandb callbacks\n",
    "    if CFG.wandb:\n",
    "        callbacks += wb_callbacks\n",
    "        \n",
    "    # train\n",
    "    print('Training...')\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=CFG.epochs if not CFG.debug else 2, \n",
    "        callbacks = callbacks, \n",
    "        steps_per_epoch=len(train_paths)/CFG.batch_size//REPLICAS,\n",
    "        validation_data=val_ds, \n",
    "        verbose=CFG.verbose\n",
    "    )\n",
    "    \n",
    "    # store best results\n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    best_loss = history.history['val_loss'][best_epoch]\n",
    "    best_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\n",
    "    best_acc_extra = history.history['val_extra_accuracy'][best_epoch]\n",
    "    best_acc_liver = history.history['val_liver_accuracy'][best_epoch]\n",
    "    best_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\n",
    "    best_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n",
    "\n",
    "    # Find mean accuracy\n",
    "    best_acc = np.mean([best_acc_bowel, best_acc_extra, \n",
    "                        best_acc_liver, best_acc_kidney, best_acc_spleen])\n",
    "\n",
    "    print(f'\\n{\"=\"*17} FOLD {fold} RESULTS {\"=\"*17}')\n",
    "    print(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\n",
    "    print('ORGAN Acc:')\n",
    "    print(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\n",
    "    print(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\n",
    "    print(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\n",
    "    print(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\n",
    "    print(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')\n",
    "\n",
    "    print(f'{\"=\"*50}\\n')\n",
    "\n",
    "    scores.append([best_loss, best_acc, \n",
    "                   best_acc_bowel, best_acc_extra, \n",
    "                   best_acc_liver, best_acc_kidney, best_acc_spleen])\n",
    "    \n",
    "    # log best result on wandb & plot\n",
    "    if CFG.wandb:\n",
    "        log_wandb(fold) # log\n",
    "        wandb.run.finish() # finish the run\n",
    "        display(ipd.IFrame(run.url, width=1080, height=720)) # show wandb dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate OOF Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-07-28T05:24:31.894816Z",
     "iopub.status.busy": "2023-07-28T05:24:31.894432Z",
     "iopub.status.idle": "2023-07-28T05:24:31.902933Z",
     "shell.execute_reply": "2023-07-28T05:24:31.901905Z",
     "shell.execute_reply.started": "2023-07-28T05:24:31.894782Z"
    }
   },
   "outputs": [],
   "source": [
    "# overall oof pF1\n",
    "oof_loss, oof_acc, oof_acc_bowel, oof_acc_extra, oof_acc_liver, oof_acc_kidney, oof_acc_spleen = np.array(scores).mean(axis=0)\n",
    "\n",
    "print(f'\\n{\"=\"*15} OVERALL OOF RESULTS {\"=\"*15}')\n",
    "print(f'>>>> OOF BEST Loss : {oof_loss:.3f}\\n>>>> OOF BEST Acc  : {oof_acc:.3f}\\n')\n",
    "print('ORGAN OOF Acc:')\n",
    "print(f'  >>>> {\"Bowel\".ljust(15)} : {oof_acc_bowel:.3f}')\n",
    "print(f'  >>>> {\"Extravasation\".ljust(15)} : {oof_acc_extra:.3f}')\n",
    "print(f'  >>>> {\"Liver\".ljust(15)} : {oof_acc_liver:.3f}')\n",
    "print(f'  >>>> {\"Kidney\".ljust(15)} : {oof_acc_kidney:.3f}')\n",
    "print(f'  >>>> {\"Spleen\".ljust(15)} : {oof_acc_spleen:.3f}')\n",
    "print(f'{\"=\"*50}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-28T05:24:29.222554Z",
     "iopub.status.idle": "2023-07-28T05:24:29.223374Z",
     "shell.execute_reply": "2023-07-28T05:24:29.223135Z",
     "shell.execute_reply.started": "2023-07-28T05:24:29.223112Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "1. [RANZCR: EfficientNet TPU Training](https://www.kaggle.com/xhlulu/ranzcr-efficientnet-tpu-training)\n",
    "1. [Triple Stratified KFold with TFRecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 6863140,
     "sourceId": 52254,
     "sourceType": "competition"
    },
    {
     "datasetId": 3563333,
     "sourceId": 6205800,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3567114,
     "sourceId": 6211844,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "capstone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "\n",
    "This notebook, I use  **Convolutional Neural Network (CNN)** model using Keras (Core and CV) on the RSNA 2023 Abdominal Trauma Detection dataset.\n",
    "\n",
    "\n",
    "In this notebook I will do:\n",
    "\n",
    "* Loading the tensor data.\n",
    "* Applying augmentations inside the data pipeline.\n",
    "* Create the model using KerasCV presets.\n",
    "* Train the model.\n",
    "* Visualize the training plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports\n",
    "\n",
    "We will need KerasCV for this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# You can use `tensorflow`, `pytorch`, `jax` here\n",
    "# KerasCore makes the notebook backend agnostic :)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras_cv\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "A particularly good practise is to have a configuration class for your notebooks. This not only keeps your configurations all at a single place but also becomes handy to map the configs to the performance of the model.\n",
    "\n",
    "Please play around with the configurations and see how the performance of the model changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on some observations\n",
    "\n",
    "Reference Notebook: https://www.kaggle.com/code/aritrag/eda-train-csv\n",
    "\n",
    "1. Class Dependencies: Refers to inherent relationships between classes in the analysis.\n",
    "2. Complementarity: `bowel_injury` and `bowel_healthy`, as well as `extravasation_injury` and `extravasation_healthy`, are perfectly complementary, with their sum always equal to 1.0.\n",
    "3. Simplification: For the model, only `{bowel/extravasation}_injury` will be included, and the corresponding healthy status can be calculated using a sigmoid function.\n",
    "4. Softmax: `{kidney/liver/spleen}_{healthy/low/high}` classifications are softmaxed, ensuring their combined probabilities sum up to 1.0 for each organ, simplifying the model while preserving essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:38.858021Z",
     "iopub.status.busy": "2023-08-08T15:29:38.857272Z",
     "iopub.status.idle": "2023-08-08T15:29:38.869335Z",
     "shell.execute_reply": "2023-08-08T15:29:38.867421Z",
     "shell.execute_reply.started": "2023-08-08T15:29:38.857982Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = [256, 256]\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10\n",
    "    TARGET_COLS  = [\n",
    "        \"bowel_injury\", \"extravasation_injury\",\n",
    "        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n",
    "        \"liver_healthy\", \"liver_low\", \"liver_high\",\n",
    "        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n",
    "    ]\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "We would want this notebook to have reproducible results. Here we set the seed for all the random algorithms so that we can reproduce the experiments each time exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:38.87478Z",
     "iopub.status.busy": "2023-08-08T15:29:38.87443Z",
     "iopub.status.idle": "2023-08-08T15:29:38.916715Z",
     "shell.execute_reply": "2023-08-08T15:29:38.915772Z",
     "shell.execute_reply.started": "2023-08-08T15:29:38.874745Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(seed=config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset provided in the competition consists of DICOM images. We will not be training on the DICOM images, rather would work on PNG image which are extracted from the DICOM format.\n",
    "\n",
    "[A helpful resource on the conversion of DICOM to PNG](https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:39.578347Z",
     "iopub.status.busy": "2023-08-08T15:29:39.577622Z",
     "iopub.status.idle": "2023-08-08T15:29:39.5831Z",
     "shell.execute_reply": "2023-08-08T15:29:39.581946Z",
     "shell.execute_reply.started": "2023-08-08T15:29:39.578309Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = f\"/kaggle/input/rsna-atd-512x512-png-v2-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Data\n",
    "\n",
    "The `train.csv` file contains the following meta information:\n",
    "\n",
    "- `patient_id`: A unique ID code for each patient.\n",
    "- `series_id`: A unique ID code for each scan.\n",
    "- `instance_number`: The image number within the scan. The lowest instance number for many series is above zero as the original scans were cropped to the abdomen.\n",
    "- `[bowel/extravasation]_[healthy/injury]`: The two injury types with binary targets.\n",
    "- `[kidney/liver/spleen]_[healthy/low/high]`: The three injury types with three target levels.\n",
    "- `any_injury`: Whether the patient had any injury at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:41.942831Z",
     "iopub.status.busy": "2023-08-08T15:29:41.942188Z",
     "iopub.status.idle": "2023-08-08T15:29:42.103063Z",
     "shell.execute_reply": "2023-08-08T15:29:42.10187Z",
     "shell.execute_reply.started": "2023-08-08T15:29:41.942794Z"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "dataframe = pd.read_csv(f\"{BASE_PATH}/train.csv\")\n",
    "dataframe[\"image_path\"] = f\"{BASE_PATH}/train_images\"\\\n",
    "                    + \"/\" + dataframe.patient_id.astype(str)\\\n",
    "                    + \"/\" + dataframe.series_id.astype(str)\\\n",
    "                    + \"/\" + dataframe.instance_number.astype(str) +\".png\"\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the training dataset into train and validation. This is a common practise in the Machine Learning pipelines. We not only want to train our model, but also want to validate it's training.\n",
    "\n",
    "A small catch here is that the training and validation data should have an aligned data distribution. Here we handle that by grouping the lables and then splitting the dataset. This ensures an aligned data distribution between the training and the validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:43.941183Z",
     "iopub.status.busy": "2023-08-08T15:29:43.940815Z",
     "iopub.status.idle": "2023-08-08T15:29:44.128396Z",
     "shell.execute_reply": "2023-08-08T15:29:44.127242Z",
     "shell.execute_reply.started": "2023-08-08T15:29:43.941151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle the split for each group\n",
    "def split_group(group, test_size=0.2):\n",
    "    if len(group) == 1:\n",
    "        return (group, pd.DataFrame()) if np.random.rand() < test_size else (pd.DataFrame(), group)\n",
    "    else:\n",
    "        return train_test_split(group, test_size=test_size, random_state=42)\n",
    "\n",
    "# Initialize the train and validation datasets\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through the groups and split them, handling single-sample groups\n",
    "for _, group in dataframe.groupby(config.TARGET_COLS):\n",
    "    train_group, val_group = split_group(group)\n",
    "    train_data = pd.concat([train_data, train_group], ignore_index=True)\n",
    "    val_data = pd.concat([val_data, val_group], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:29:46.267321Z",
     "iopub.status.busy": "2023-08-08T15:29:46.266896Z",
     "iopub.status.idle": "2023-08-08T15:29:46.275503Z",
     "shell.execute_reply": "2023-08-08T15:29:46.274524Z",
     "shell.execute_reply.started": "2023-08-08T15:29:46.267287Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline /w tf.data\n",
    "\n",
    "Here we build the data pipeline using `tf.data`. Using `tf.data` we can map out data to an augmentation pipeline simple by using the ` map` API.\n",
    "\n",
    "Adding augmentations to the data pipeline is as simple as adding a layer into the list of layers that the `Augmenter` processes.\n",
    "\n",
    "Reference: https://keras.io/api/keras_cv/layers/augmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image_and_label(image_path, label):\n",
    "    file_bytes = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(file_bytes, channels=3, dtype=tf.uint8)\n",
    "    image = tf.image.resize(image, config.IMAGE_SIZE, method=\"bilinear\")\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    label = tf.cast(label, tf.float32)\n",
    "    #         bowel       fluid       kidney      liver       spleen\n",
    "    labels = (label[0:1], label[1:2], label[2:5], label[5:8], label[8:11])\n",
    "    \n",
    "    return (image, labels)\n",
    "\n",
    "\n",
    "def apply_augmentation(images, labels):\n",
    "    augmenter = keras_cv.layers.Augmenter(\n",
    "        [\n",
    "            keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "            keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    return (augmenter(images), labels)\n",
    "\n",
    "\n",
    "def build_dataset(image_paths, labels):\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        .map(decode_image_and_label, num_parallel_calls=config.AUTOTUNE)\n",
    "        .shuffle(config.BATCH_SIZE * 10)\n",
    "        .batch(config.BATCH_SIZE)\n",
    "        .map(apply_augmentation, num_parallel_calls=config.AUTOTUNE)\n",
    "        .prefetch(config.AUTOTUNE)\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m paths  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mimage_path\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m train_data[config\u001b[38;5;241m.\u001b[39mTARGET_COLS]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m build_dataset(image_paths\u001b[38;5;241m=\u001b[39mpaths, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "paths  = train_data.image_path.tolist()\n",
    "labels = train_data[config.TARGET_COLS].values\n",
    "\n",
    "ds = build_dataset(image_paths=paths, labels=labels)\n",
    "images, labels = next(iter(ds))\n",
    "images.shape, [label.shape for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:30:16.956934Z",
     "iopub.status.busy": "2023-08-08T15:30:16.954134Z",
     "iopub.status.idle": "2023-08-08T15:30:17.583577Z",
     "shell.execute_reply": "2023-08-08T15:30:17.582393Z",
     "shell.execute_reply.started": "2023-08-08T15:30:16.956896Z"
    }
   },
   "outputs": [],
   "source": [
    "# No more customizing your plots by hand, KerasCV has your back ;)\n",
    "keras_cv.visualization.plot_image_gallery(\n",
    "    images=images,\n",
    "    value_range=(0, 1),\n",
    "    rows=2,\n",
    "    cols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "\n",
    "We are going to load a pretrained model from the [list of avaiable backbones in KerasCV](https://keras.io/api/keras_cv/models/backbones/). We are using the `ResNetBackbone` as our backbone. The practise of using a pretrained model and finetuning it to a specific dataset is prevalent in the DL community.\n",
    "\n",
    "We use the [Functional API](https://keras.io/guides/functional_api/) of Keras to build the model. The design of the model would be such that we input a single image and we get different heads for the various predictions we need (kidney, spleen...).\n",
    "\n",
    "We have also added a Learning Rate scheduler for you to work with. When an athlete trains, the first step is always to warm up. We take a similar approach to training our models. We warm up with model where the learning rate increses from the initial LR to a higher LR. After the warmup stage we provide a decay algorithm (cosine here). A list of all the learning rate scheduler can be found [here](https://keras.io/api/optimizers/learning_rate_schedules/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:33:21.162423Z",
     "iopub.status.busy": "2023-08-08T15:33:21.16204Z",
     "iopub.status.idle": "2023-08-08T15:33:21.178363Z",
     "shell.execute_reply": "2023-08-08T15:33:21.177299Z",
     "shell.execute_reply.started": "2023-08-08T15:33:21.162384Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(warmup_steps, decay_steps):\n",
    "    # Define Input\n",
    "    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE)\n",
    "    \n",
    "    # Define Backbone\n",
    "    backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n",
    "    backbone.include_rescaling = False\n",
    "    x = backbone(inputs)\n",
    "    \n",
    "    # GAP to get the activation maps\n",
    "    gap = keras.layers.GlobalAveragePooling2D()\n",
    "    x = gap(x)\n",
    "\n",
    "    # Define 'necks' for each head\n",
    "    x_bowel = keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_extra = keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_liver = keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_kidney = keras.layers.Dense(32, activation='silu')(x)\n",
    "    x_spleen = keras.layers.Dense(32, activation='silu')(x)\n",
    "\n",
    "    # Define heads\n",
    "    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid')(x_bowel) # use sigmoid to convert predictions to [0-1]\n",
    "    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid')(x_extra) # use sigmoid to convert predictions to [0-1]\n",
    "    out_liver = keras.layers.Dense(3, name='liver', activation='softmax')(x_liver) # use softmax for the liver head\n",
    "    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax')(x_kidney) # use softmax for the kidney head\n",
    "    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax')(x_spleen) # use softmax for the spleen head\n",
    "    \n",
    "    # Concatenate the outputs\n",
    "    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n",
    "\n",
    "    # Create model\n",
    "    print(\"[INFO] Building the model...\")\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Cosine Decay\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.0,\n",
    "        warmup_target=1e-3,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n",
    "    loss = {\n",
    "        \"bowel\":keras.losses.BinaryCrossentropy(),\n",
    "        \"extra\":keras.losses.BinaryCrossentropy(),\n",
    "        \"liver\":keras.losses.CategoricalCrossentropy(),\n",
    "        \"kidney\":keras.losses.CategoricalCrossentropy(),\n",
    "        \"spleen\":keras.losses.CategoricalCrossentropy(),\n",
    "    }\n",
    "    metrics = {\n",
    "        \"bowel\":[\"accuracy\"],\n",
    "        \"extra\":[\"accuracy\"],\n",
    "        \"liver\":[\"accuracy\"],\n",
    "        \"kidney\":[\"accuracy\"],\n",
    "        \"spleen\":[\"accuracy\"],\n",
    "    }\n",
    "    print(\"[INFO] Compiling the model...\")\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "      loss=loss,\n",
    "      metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with \"model.fit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T15:33:42.356989Z",
     "iopub.status.busy": "2023-08-08T15:33:42.356627Z",
     "iopub.status.idle": "2023-08-08T15:33:44.307542Z",
     "shell.execute_reply": "2023-08-08T15:33:44.306444Z",
     "shell.execute_reply.started": "2023-08-08T15:33:42.356959Z"
    }
   },
   "outputs": [],
   "source": [
    "# get image_paths and labels\n",
    "print(\"[INFO] Building the dataset...\")\n",
    "train_paths = train_data.image_path.values; train_labels = train_data[config.TARGET_COLS].values.astype(np.float32)\n",
    "valid_paths = val_data.image_path.values; valid_labels = val_data[config.TARGET_COLS].values.astype(np.float32)\n",
    "\n",
    "# train and valid dataset\n",
    "train_ds = build_dataset(image_paths=train_paths, labels=train_labels)\n",
    "val_ds = build_dataset(image_paths=valid_paths, labels=valid_labels)\n",
    "\n",
    "total_train_steps = train_ds.cardinality().numpy() * config.BATCH_SIZE * config.EPOCHS\n",
    "warmup_steps = int(total_train_steps * 0.10)\n",
    "decay_steps = total_train_steps - warmup_steps\n",
    "\n",
    "print(f\"{total_train_steps=}\")\n",
    "print(f\"{warmup_steps=}\")\n",
    "print(f\"{decay_steps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2023-08-08T15:33:58.866814Z",
     "iopub.status.busy": "2023-08-08T15:33:58.86636Z",
     "iopub.status.idle": "2023-08-08T16:30:56.183647Z",
     "shell.execute_reply": "2023-08-08T16:30:56.182432Z",
     "shell.execute_reply.started": "2023-08-08T15:33:58.866779Z"
    }
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "print(\"[INFO] Building the model...\")\n",
    "model = build_model(warmup_steps, decay_steps)\n",
    "\n",
    "# train\n",
    "print(\"[INFO] Training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=config.EPOCHS,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:32:25.431674Z",
     "iopub.status.busy": "2023-08-08T16:32:25.430297Z",
     "iopub.status.idle": "2023-08-08T16:32:26.982262Z",
     "shell.execute_reply": "2023-08-08T16:32:26.981295Z",
     "shell.execute_reply.started": "2023-08-08T16:32:25.431627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a 3x2 grid for the subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(5, 15))\n",
    "\n",
    "# Flatten axes to iterate through them\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the metrics and plot them\n",
    "for i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n",
    "    # Plot training accuracy\n",
    "    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n",
    "    # Plot validation accuracy\n",
    "    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:32:47.178362Z",
     "iopub.status.busy": "2023-08-08T16:32:47.17799Z",
     "iopub.status.idle": "2023-08-08T16:32:47.475977Z",
     "shell.execute_reply": "2023-08-08T16:32:47.475045Z",
     "shell.execute_reply.started": "2023-08-08T16:32:47.178332Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:33:15.41817Z",
     "iopub.status.busy": "2023-08-08T16:33:15.416975Z",
     "iopub.status.idle": "2023-08-08T16:33:15.429597Z",
     "shell.execute_reply": "2023-08-08T16:33:15.428289Z",
     "shell.execute_reply.started": "2023-08-08T16:33:15.418116Z"
    }
   },
   "outputs": [],
   "source": [
    "# store best results\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "best_loss = history.history['val_loss'][best_epoch]\n",
    "best_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\n",
    "best_acc_extra = history.history['val_extra_accuracy'][best_epoch]\n",
    "best_acc_liver = history.history['val_liver_accuracy'][best_epoch]\n",
    "best_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\n",
    "best_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n",
    "\n",
    "# Find mean accuracy\n",
    "best_acc = np.mean(\n",
    "    [best_acc_bowel,\n",
    "     best_acc_extra,\n",
    "     best_acc_liver,\n",
    "     best_acc_kidney,\n",
    "     best_acc_spleen\n",
    "])\n",
    "\n",
    "\n",
    "print(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\n",
    "print('ORGAN Acc:')\n",
    "print(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\n",
    "print(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\n",
    "print(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\n",
    "print(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\n",
    "print(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:32:59.237398Z",
     "iopub.status.busy": "2023-08-08T16:32:59.236948Z",
     "iopub.status.idle": "2023-08-08T16:33:00.540043Z",
     "shell.execute_reply": "2023-08-08T16:33:00.538877Z",
     "shell.execute_reply.started": "2023-08-08T16:32:59.237343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"rsna-atd.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. Please refer to the [Inference Notebook](https://www.kaggle.com/code/aritrag/kerascv-starter-notebook-infer) to learn about submitting to the competition\n",
    "2. Dive deep into [KerasCV](https://github.com/keras-team/keras-cv) and [KerasCore](https://github.com/keras-team/keras-core)\n",
    "\n",
    "# Credits\n",
    "\n",
    "This notebook was forked from https://www.kaggle.com/code/awsaf49/rsna-atd-cnn-tpu-train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "capstone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

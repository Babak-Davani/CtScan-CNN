{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6099d5-7c1c-4352-86a5-5d3fe080f705",
   "metadata": {},
   "source": [
    "# This is my first CNN Model using PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "666ca6b6-86ce-4c13-8937-d357f8c4e646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46faf74d-ae99-4473-a4a1-2f967483c44f",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "First, we'll define the code for loading and preprocessing the data. This involves normalizing the tensors and splitting the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe795d3-eb14-489a-af21-2e7155dd693f",
   "metadata": {},
   "source": [
    "### Dataset Preparation with Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1267a54-5796-434c-9245-a918cd327b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define my path in hard drive\n",
    "# Windows\n",
    "data_dir = \"S:/Capston/data2\"\n",
    "# My 3D tensor files directory\n",
    "SAVE_FOLDER = \"S:/Capston/data4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fad47-4380-49e7-b7df-96713cb30b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'PyTorch_CNN'\n",
    "\n",
    "    # size of the image\n",
    "    img_size = [128, 128]\n",
    "\n",
    "\n",
    "    # batch_size and epochs\n",
    "    batch_size = 48\n",
    "    epochs = 10\n",
    "\n",
    "    # loss\n",
    "    loss      = 'BCE & CCE'  # BCE, Focal\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = 'Adam'\n",
    "\n",
    "    # augmentation\n",
    "    augment   = True\n",
    "\n",
    "    # scale-shift-rotate-shear\n",
    "    transform = 0.90  # transform prob\n",
    "    fill_mode = 'constant'\n",
    "    rot    = 2.0\n",
    "    shr    = 2.0\n",
    "    hzoom  = 50.0\n",
    "    wzoom  = 50.0\n",
    "    hshift = 10.0\n",
    "    wshift = 10.0\n",
    "\n",
    "    # flip\n",
    "    hflip = True\n",
    "    vflip = True\n",
    "\n",
    "    # clip\n",
    "    clip = False\n",
    "\n",
    "    # lr-scheduler\n",
    "    scheduler   = 'cosine' # cosine\n",
    "\n",
    "    # dropout\n",
    "    drop_prob   = 0.6\n",
    "    drop_cnt    = 5\n",
    "    drop_size   = 0.05\n",
    "    \n",
    "    # cut-mix-up\n",
    "    mixup_prob = 0.0\n",
    "    mixup_alpha = 0.5\n",
    "    \n",
    "    cutmix_prob = 0.0\n",
    "    cutmix_alpha = 2.5\n",
    "\n",
    "    # pixel-augment\n",
    "    pixel_aug = 0.90  # prob of pixel_aug\n",
    "    sat  = [0.7, 1.3]\n",
    "    cont = [0.8, 1.2]\n",
    "    bri  = 0.15\n",
    "    hue  = 0.05\n",
    "\n",
    "    # test-time augs\n",
    "    tta = 1\n",
    "    \n",
    "    # target column\n",
    "    target_col  = [ \"bowel_injury\", \"extravasation_injury\", \"kidney_healthy\", \"kidney_low\",\n",
    "                   \"kidney_high\", \"liver_healthy\", \"liver_low\", \"liver_high\",\n",
    "                   \"spleen_healthy\", \"spleen_low\", \"spleen_high\"] # not using \"bowel_healthy\" & \"extravasation_healthy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfc394e-a8a6-4fca-b223-1f730b6abe9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3147, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV and making dataframe\n",
    "df = pd.read_csv(os.path.join(data_dir,'patients_meta_plus_path.csv'))\n",
    "# sanity check\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970a181e-d61f-401e-a690-2333355c40ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "      <th>any_injury</th>\n",
       "      <th>paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10004</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S:\\Capston\\data4\\10004.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S:\\Capston\\data4\\10005.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S:\\Capston\\data4\\10007.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S:\\Capston\\data4\\10026.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10051</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>S:\\Capston\\data4\\10051.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0       10004              1             0                      0   \n",
       "1       10005              1             0                      1   \n",
       "2       10007              1             0                      1   \n",
       "3       10026              1             0                      1   \n",
       "4       10051              1             0                      1   \n",
       "\n",
       "   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0                     1               0           1            0   \n",
       "1                     0               1           0            0   \n",
       "2                     0               1           0            0   \n",
       "3                     0               1           0            0   \n",
       "4                     0               1           0            0   \n",
       "\n",
       "   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0              1          0           0               0           0   \n",
       "1              1          0           0               1           0   \n",
       "2              1          0           0               1           0   \n",
       "3              1          0           0               1           0   \n",
       "4              1          0           0               0           1   \n",
       "\n",
       "   spleen_high  any_injury                      paths  \n",
       "0            1           1  S:\\Capston\\data4\\10004.pt  \n",
       "1            0           0  S:\\Capston\\data4\\10005.pt  \n",
       "2            0           0  S:\\Capston\\data4\\10007.pt  \n",
       "3            0           0  S:\\Capston\\data4\\10026.pt  \n",
       "4            0           1  S:\\Capston\\data4\\10051.pt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43cdb22-5384-4cd2-8600-cabd307d7af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['patient_id', 'bowel_healthy', 'bowel_injury', 'extravasation_healthy',\n",
       "       'extravasation_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
       "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
       "       'spleen_low', 'spleen_high', 'any_injury', 'paths'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12e9e53-e71d-4737-9220-ab03d9c2867a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['bowel_healthy', 'extravasation_healthy',\n",
    "       'extravasation_injury','any_injury'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1831f951-08ff-4a88-9362-47390bd9fb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "remain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)  # 20% data for testing\n",
    "train_df, val_df = train_test_split(remain_df, test_size=0.25, random_state=42)  # 60% training, 20% validation\n",
    "\n",
    "# Balancing the training dataset\n",
    "class_counts = train_df[['bowel_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
    "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
    "       'spleen_low', 'spleen_high']].sum().values\n",
    "num_samples = len(train_df)\n",
    "weights = 1. / class_counts\n",
    "samples_weights = weights[train_df[['bowel_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
    "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
    "       'spleen_low', 'spleen_high']].values.argmax(axis=1)]\n",
    "sampler = WeightedRandomSampler(samples_weights, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d4f962-d343-4330-8f23-1fd2291d17f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): DataFrame containing the file paths and labels.\n",
    "            root_dir (string): Directory with all the tensor files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, -1])\n",
    "        # ... load the image ...\n",
    "        image = torch.load(img_name)\n",
    "        # Add a channel dimension\n",
    "        image = image.unsqueeze(0)  # This adds a channel dimension\n",
    "        labels = self.dataframe.iloc[idx, 1:-1].values\n",
    "        labels = torch.from_numpy(labels.astype('float')).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = CTScanDataset(train_df, SAVE_FOLDER)\n",
    "val_dataset = CTScanDataset(val_df, SAVE_FOLDER)\n",
    "test_dataset = CTScanDataset(test_df, SAVE_FOLDER)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f22bc2-76de-4f7d-9cab-b5c4a4291298",
   "metadata": {},
   "source": [
    "## 2. CNN Model Architecture\n",
    "Next, we'll define a simple 3D CNN model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9908682-785c-45b9-b60d-1ebcebcc177e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple3DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, 3)  # Set input channels to 1\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        self.conv2 = nn.Conv3d(16, 32, 3)\n",
    "        self.fc1 = nn.Linear(32 * 30 * 30 * 30, 120)  # Adjust the input features number_of_neurons=120\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # Adjust the output features based on number of labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = Simple3DCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53dcd52-90ad-4f5e-9633-4d8cdc329974",
   "metadata": {},
   "source": [
    "## 3. Training the Model\n",
    "Finally, we'll set up a basic training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d8b072b-4aca-41a9-a620-271e3b6d47b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████| 472/472 [46:10<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3907, Accuracy: 1.3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████| 472/472 [40:36<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0634, Accuracy: 0.1287\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # or another appropriate loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    predicted = outputs.round()  # or use a threshold, e.g., outputs > 0.5\n",
    "    correct = (predicted == labels).float()  # convert to float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# Training loop with accuracy tracking\n",
    "num_epochs = 2  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_accuracy += calculate_accuracy(outputs, labels)\n",
    "\n",
    "    # Calculate average loss and accuracy over an epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = total_accuracy / len(train_loader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40b21b-7e49-449e-b93d-8a1148d47700",
   "metadata": {},
   "source": [
    "## 4.Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25c43ba3-cbfb-4b80-9715-cdb8335eb4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 1., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n",
      "labels tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 1., 0.]])\n",
      "Preedict tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Convert outputs to predictions: apply a threshold for multi-label classification\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mSimple3DCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:244\u001b[0m, in \u001b[0;36mMaxPool3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\babak\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\nn\\functional.py:877\u001b[0m, in \u001b[0;36m_max_pool3d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "label_names= ['bowel_injury', 'kidney_healthy', 'kidney_low', 'kidney_high',\n",
    "       'liver_healthy', 'liver_low', 'liver_high', 'spleen_healthy',\n",
    "       'spleen_low', 'spleen_high']\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters\n",
    "label_correct = torch.zeros(10)  # Assuming 10 labels\n",
    "label_total = torch.zeros(10)\n",
    "\n",
    "# No gradient is needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Convert outputs to predictions: apply a threshold for multi-label classification\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        for i in range(10):  # Loop over each label\n",
    "            label_total[i] += len(labels[:, i])  # Total instances of each label\n",
    "            label_correct[i] += (predicted[:, i] == labels[:, i]).sum()  # Correct predictions for each label\n",
    "\n",
    "# Calculate the accuracy for each label\n",
    "label_accuracy = 100 * label_correct / label_total\n",
    "\n",
    "# Print accuracy for each label\n",
    "for i in range(10):\n",
    "    print(f'Accuracy for {label_names[i]}: {label_accuracy[i]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e12c97b-266f-4505-8c7c-b40d43562978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving my entire model\n",
    "torch.save(model, \"S:/Capston/data/model/pytorch_2epoch_4batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbdf13-2c69-4b4c-91e3-b139b21e25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a figure and two subplots (axes) side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Flatten the array of axes to easily index each subplot\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Plotting training and validation accuracies on the first subplot\n",
    "ax[0].plot(train_accuracies, label='Train Accuracy')  # Plot training accuracy\n",
    "ax[0].plot(val_accuracies, label='Val Accuracy')     # Plot validation accuracy\n",
    "ax[0].set_title('Model Accuracy')                    # Set title for the first subplot\n",
    "ax[0].set_xlabel('Epochs')                           # Set x-axis label for the first subplot\n",
    "ax[0].set_ylabel('Accuracy')                         # Set y-axis label for the first subplot\n",
    "ax[0].legend()                                       # Display legend for the first subplot\n",
    "\n",
    "# Plotting training and validation losses on the second subplot\n",
    "ax[1].plot(train_losses, label='Train Loss')         # Plot training loss\n",
    "ax[1].plot(val_losses, label='Val Loss')             # Plot validation loss\n",
    "ax[1].set_title('Model Loss')                        # Set title for the second subplot\n",
    "ax[1].set_xlabel('Epochs')                           # Set x-axis label for the second subplot\n",
    "ax[1].set_ylabel('Loss')                             # Set y-axis label for the second subplot\n",
    "ax[1].legend()                                       # Display legend for the second subplot\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "capstone_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
